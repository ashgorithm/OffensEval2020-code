{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "Eng Task C - Ensemble DistilRoberta AttnMask Dropout.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_dUF2evouow",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cozek/OffensEval2020-code/blob/master/notebooks/Eng%20Task%20C%20-%20Ensemble%20DistilRoberta%20AttnMask%20Dropout.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7Uo50Chouox",
        "colab_type": "text"
      },
      "source": [
        "# Import Libraries\n",
        "\n",
        "At the time of our work, we used the following library versions\n",
        "- numpy 1.18.1\n",
        "- pandas 1.0.1\n",
        "- torch 1.2.0\n",
        "- Cuda 10.0\n",
        "- python 3.7.0\n",
        "- sklearn 0.22.1\n",
        "- tqdm 4.42.1\n",
        "- nltk 3.4.5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFPGTXG3ouox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/cozek/OffensEval2020-code/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpGHYSkhouo0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers\n",
        "!pip install /content/transformers/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6om7VnoNouo3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/OffensEval2020-code/src/')\n",
        "import collections\n",
        "from typing import Callable\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "import pandas as pd\n",
        "from tqdm import notebook\n",
        "import importlib\n",
        "import pprint\n",
        "import nltk\n",
        "import datetime\n",
        "import os\n",
        "from argparse import Namespace\n",
        "\n",
        "from collections import Counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyUPjzykouo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import utils.general as general_utils\n",
        "import utils.transformer.data as transformer_data_utils\n",
        "import utils.transformer.general as transformer_general_utils\n",
        "general_utils.set_seed_everywhere()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoJmxSDPouo9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(level=logging.INFO) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ehv7SLoouo_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "torch.__version__ # we used version 1.2.0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkS9WQy2oupC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import RAdam and Lookahead\n",
        "from radam.radam import RAdam\n",
        "from lookahead.optimizer import Lookahead\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71wul4V7oupF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import RobertaTokenizer, RobertaModel, RobertaForSequenceClassification"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoAlBoFYoupH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " args = Namespace(\n",
        "        #use cuda by default\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    \n",
        "        #set batch size and number of epochs\n",
        "        batch_size = 32,\n",
        "        num_epochs = 20,\n",
        "    \n",
        "        #set the learning rate\n",
        "        learning_rate = 0.0001,\n",
        "\n",
        "        #location of the train, dev and test csv\n",
        "        train_val_csv = '/content/OffensEval2020-code/data/eng/task_c_tiny.zip',\n",
        "        test_csv = '/content/OffensEval2020-code/data/test_data/test_a_tweets.tsv',\n",
        "    \n",
        "        #directory to save our models at\n",
        "        directory = './models/', \n",
        "        model_name = 'roberta_attn_trac_task_a.pt',\n",
        "     \n",
        "        date = datetime.datetime.now().strftime(\"%a_%d_%b_%Y/\"),\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt5X-gWsoupL",
        "colab_type": "text"
      },
      "source": [
        "## Model save location"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMOYDefpoupM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "directory = args.directory + args.date\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "args.directory = directory\n",
        "print(args.directory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thSqAbT3oupP",
        "colab_type": "text"
      },
      "source": [
        "## Load presplit dataset portion\n",
        "```\n",
        "Labelled as\n",
        "\n",
        "IND = 0\n",
        "GRP = 1\n",
        "OTH = 2\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WazWg4zoupP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_df_task_c = pd.read_csv(args.train_val_csv, compression='zip')\n",
        "print(data_df_task_c.label.value_counts())\n",
        "print(data_df_task_c.split.value_counts())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dV3P48EphCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_df_task_c.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkUZ5O46oupS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with pd.option_context('display.max_colwidth', -1): \n",
        "    print(data_df_task_c[['text','label']].sample(5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZrYLTrxoupU",
        "colab_type": "text"
      },
      "source": [
        "## Importing the Roberta Tokeniker and Punkt sentence tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtFADHnToupV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RobertaPreprocessor():\n",
        "    def __init__(self,transformer_tokenizer,sentence_detector):\n",
        "        self.transformer_tokenizer = transformer_tokenizer\n",
        "        self.sentence_detector = sentence_detector\n",
        "        self.bos_token = transformer_tokenizer.bos_token\n",
        "        self.sep_token = ' ' + transformer_tokenizer.sep_token + ' '\n",
        "    def add_special_tokens(self, text):\n",
        "        sentences = self.sentence_detector.tokenize(text)\n",
        "        eos_added_text  = self.sep_token.join(sentences) \n",
        "        return self.bos_token +' '+ eos_added_text + ' ' + self.transformer_tokenizer.sep_token"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar4EbV4BoupX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -c 'import nltk; nltk.download(\"punkt\")'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "6RlaXDBtoupY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "roberta_tokenizer = tokenizer = RobertaTokenizer.from_pretrained('distilroberta-base')\n",
        "punkt_sentence_detector = nltk.data.load('tokenizers/punkt/english.pickle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKFM-0bqoupb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "roberta_preproc = RobertaPreprocessor(roberta_tokenizer, punkt_sentence_detector)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDq61UIRoupf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#apply the preprocessor on the exploded dataframe\n",
        "data_df_task_c['text'] = data_df_task_c['text'].map(roberta_preproc.add_special_tokens)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kocu45Xtoupj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with pd.option_context('display.max_colwidth', -1): \n",
        "    print(data_df_task_c[['text','label']].sample(5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NOwcyecoupm",
        "colab_type": "text"
      },
      "source": [
        "### Implement Attention Mask Dropout in the vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyQMayxOoupn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleVectorizer():\n",
        "    def __init__(self,tokenizer: Callable, max_seq_len: int):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tokenizer (Callable): transformer tokenizer\n",
        "            max_seq_len (int): Maximum sequence lenght \n",
        "        \"\"\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self._max_seq_len = max_seq_len\n",
        "\n",
        "    def vectorize(self,text :str):\n",
        "        \n",
        "        encoded = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=False, #already added by preproc\n",
        "            max_length = self._max_seq_len,\n",
        "            pad_to_max_length = True,\n",
        "        )\n",
        "        ids =  np.array(encoded['input_ids'], dtype=np.int64)\n",
        "        attn = np.array(encoded['attention_mask'], dtype=np.int64)\n",
        "        \n",
        "        return ids, attn\n",
        "\n",
        "class Vectorizer():\n",
        "    \"\"\"Vectorizer with Attention Mask Dropout\"\"\"\n",
        "    def __init__(self,tokenizer: Callable, max_seq_len: int ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tokenizer (Callable): transformer tokenizer\n",
        "            max_seq_len (int): Maximum sequence lenght \n",
        "        \"\"\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self._max_seq_len = max_seq_len\n",
        "\n",
        "    def vectorize(self,text :str, mask_prob: float = 0.50, mask_amount:float=0.30):\n",
        "        \"\"\"Implements Attention Mask Dropout\n",
        "        \n",
        "        Args:\n",
        "            text (str): The string to vectorize\n",
        "            mask_prob (float): Probability of the attention mask \n",
        "                dropout being applied\n",
        "            mask_amount (float): Percentage of tokens to mask\n",
        "\n",
        "        Returns:\n",
        "            ids (np.array)  : Array to token ids of the text\n",
        "            attn (np.array) : 0-1 Array of attention masks\n",
        "        \"\"\"\n",
        "\n",
        "        encoded = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=False, #already added by preproc\n",
        "            max_length = self._max_seq_len,\n",
        "            pad_to_max_length = True,\n",
        "        )\n",
        "        ids =  np.array(encoded['input_ids'], dtype=np.int64)\n",
        "        attn = np.array(encoded['attention_mask'], dtype=np.int64)\n",
        "        prob = np.random.rand(1)[0]\n",
        "        if  prob <= mask_prob:\n",
        "            len_of_sent = np.where(ids==tokenizer.pad_token_id)[0][0]\n",
        "            amount_to_mask = max(int(len_of_sent * mask_amount ) , 1)\n",
        "            ids_to_not_attend = [np.random.randint(low=0, high=len_of_sent )\n",
        "             for i in range(amount_to_mask)]\n",
        "            attn[ids_to_not_attend]=0\n",
        "            ids[ids_to_not_attend] = tokenizer.mask_token_id\n",
        "        return ids, attn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0pI_W73rkHt",
        "colab_type": "text"
      },
      "source": [
        "Attention Mask Dropout Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZTtbNz7oupp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "v = Vectorizer(roberta_tokenizer, 15) #attention maskdropout vectorizer\n",
        "sv = SimpleVectorizer(roberta_tokenizer, 15) #simple vectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx4WAmFuoups",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent = \"I am alright bro, dont worry about me\"\n",
        "_, attn_masks_dropped = v.vectorize(sent)\n",
        "attn_masks_dropped"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGb2aQcTrWxv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_, attn_masks = sv.vectorize(sent)\n",
        "attn_masks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfXDJER9wnGn",
        "colab_type": "text"
      },
      "source": [
        "###  Create the dataset class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JI0SzK1Woupw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HateDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_df: pd.DataFrame,\n",
        "        tokenizer: Callable,\n",
        "        max_seq_length:int = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_df (pandas.DataFrame): df containing the labels and text\n",
        "            tokenizer (tokenizer module for the transformer)\n",
        "        \"\"\"\n",
        "        self.data_df = data_df\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        if max_seq_length is None:\n",
        "            self._max_seq_length = self._get_max_len(data_df,tokenizer)\n",
        "        else:\n",
        "            self._max_seq_length = max_seq_length\n",
        "\n",
        "        self.train_df = self.data_df[self.data_df.split == 'train']\n",
        "        self.train_size = len(self.train_df)\n",
        "\n",
        "        self.val_df = self.data_df[self.data_df.split == 'val']\n",
        "        self.val_size = len(self.val_df)\n",
        "\n",
        "        self.test_df = self.data_df[self.data_df.split == 'test']\n",
        "        self.test_size = len(self.test_df)\n",
        "        \n",
        "        self.simple_vectorize = False,\n",
        "        self._simple_vectorizer = SimpleVectorizer(tokenizer, self._max_seq_length)\n",
        "        self._vectorizer = Vectorizer(tokenizer, self._max_seq_length)\n",
        "        \n",
        "        self._lookup_dict = {\n",
        "            'train': (self.train_df, self.train_size),\n",
        "            'val': (self.val_df, self.val_size),\n",
        "            'test': (self.test_df, self.test_size)\n",
        "        }\n",
        "\n",
        "        self.set_split('train')\n",
        "\n",
        "        class_counts = data_df.label.value_counts().to_dict()\n",
        "         #sorted on the basis of class label,eg, 0,1,2..\n",
        "        cts = sorted([(lbl,cts) for lbl,cts in class_counts.items()], key=lambda x: x[0])\n",
        "        freq = [ x[1] for x in cts ]\n",
        "        # print(freq,cts)\n",
        "        self.class_weights = 1.0/ torch.tensor(freq, dtype=torch.float32)\n",
        "    \n",
        "    def flip_simple_vectorizer(self) :\n",
        "        if self.simple_vectorize:\n",
        "            self.simple_vectorize=False\n",
        "        else:\n",
        "            self.simple_vectorize= True\n",
        "    \n",
        "    def _get_max_len(self,data_df: pd.DataFrame, tokenizer: Callable):\n",
        "        len_func = lambda x: len(self.tokenizer.encode_plus(x)['input_ids'])\n",
        "        max_len = data_df.text.map(len_func).max() \n",
        "        return max_len\n",
        "\n",
        "    def set_split(self, split=\"train\"):\n",
        "        \"\"\" selects the splits in the dataset using a column in the dataframe \"\"\"\n",
        "        self._target_split = split\n",
        "        self._target_df, self._target_size = self._lookup_dict[split]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self._target_size\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"the primary entry point method for PyTorch datasets\n",
        "        \n",
        "        Args:\n",
        "            index (int): the index to the data point \n",
        "        Returns:\n",
        "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
        "        \"\"\"\n",
        "        row = self._target_df.iloc[index]\n",
        "        \n",
        "        if self._target_split == 'train':\n",
        "            indices, attention_masks = self._vectorizer.vectorize(row.text)\n",
        "        else:\n",
        "            indices, attention_masks = self._simple_vectorizer.vectorize(row.text)\n",
        "\n",
        "        label = row.label\n",
        "        return {'x_data': indices,\n",
        "                'x_attn_mask': attention_masks,\n",
        "                'x_index': index,\n",
        "                'y_target': label}\n",
        "    \n",
        "    def get_num_batches(self, batch_size):\n",
        "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
        "        \n",
        "        Args:\n",
        "            batch_size (int)\n",
        "        Returns:\n",
        "            number of batches in the dataset\n",
        "        \"\"\"\n",
        "        return len(self) // batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGvvsTq9oupy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batches(dataset, batch_size, shuffle=True,\n",
        "                     drop_last=False, device=\"cpu\", pinned_memory = False, n_workers = 0): \n",
        "    \"\"\"\n",
        "    A generator function which wraps the PyTorch DataLoader. It will \n",
        "      ensure each tensor is on the write device location.\n",
        "    \"\"\"\n",
        "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
        "                            shuffle=shuffle, drop_last=drop_last,\n",
        "                            pin_memory= pinned_memory,\n",
        "                            num_workers = n_workers,\n",
        "                            )\n",
        "    \n",
        "    for data_dict in dataloader:\n",
        "        out_data_dict = {}\n",
        "        out_data_dict['x_data'] = data_dict['x_data'].to(\n",
        "            device, non_blocking= (True if pinned_memory else False) \n",
        "        )\n",
        "        out_data_dict['x_attn_mask'] = data_dict['x_attn_mask'].to(\n",
        "            device, non_blocking= (True if pinned_memory else False) \n",
        "        )\n",
        "        out_data_dict['x_index'] = data_dict['x_index']\n",
        "        out_data_dict['y_target'] = data_dict['y_target'].to(\n",
        "            device, non_blocking= (True if pinned_memory else False) \n",
        "        )\n",
        "        yield out_data_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqjxEQKZoup0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = HateDataset(\n",
        "    data_df = data_df_task_c,\n",
        "    tokenizer = roberta_tokenizer\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "r1S0e8djoup3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert dataset._max_seq_length <= 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faAM7TdDoup5",
        "colab_type": "text"
      },
      "source": [
        "# Initialize the Roberta model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "Sdjpj_fvoup6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = RobertaForSequenceClassification.from_pretrained(\n",
        "    'distilroberta-base',\n",
        "    num_labels=len(set(data_df_task_c.label)),\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "7VAugC07oup8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.to(args.device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nf9iX9Eoup_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "early_stopping = transformer_general_utils.EarlyStopping(patience=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjtphpU6s4LV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjFXYTcEouqB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args.num_epochs = 20\n",
        "args.batch_size = 70"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "OYfdxscNouqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "print(f'Using LR:{args.learning_rate}')\n",
        "base_optimizer = RAdam(model.parameters(), lr = args.learning_rate)\n",
        "optimizer = Lookahead(optimizer = base_optimizer, k = 5, alpha=0.5 )\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer=optimizer.optimizer, factor =0.1 ,mode='max',\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvhB0DIPouqH",
        "colab_type": "text"
      },
      "source": [
        "# Begin Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ta4xhZcdouqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_state = general_utils.make_train_state()\n",
        "train_state.keys()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "Hyp2Q5ReouqK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epoch_bar = notebook.tqdm(\n",
        "    desc = 'training_routine',\n",
        "    total = args.num_epochs,\n",
        "    position=0,\n",
        "    leave = True,\n",
        ")\n",
        "dataset.set_split('train')\n",
        "train_bar = notebook.tqdm(\n",
        "    desc = 'split=train ',\n",
        "    total=dataset.get_num_batches(args.batch_size),\n",
        "    position=0,\n",
        "    leave=True,\n",
        ")\n",
        "dataset.set_split('val')\n",
        "eval_bar = notebook.tqdm(\n",
        "    desc = 'split=eval',\n",
        "    total=dataset.get_num_batches(args.batch_size),\n",
        "    position=0,\n",
        "    leave=True,\n",
        ")\n",
        "\n",
        "old_val_acc = 0\n",
        "old_f1 = 0\n",
        "model_state = None\n",
        "for epoch_index in range(args.num_epochs):\n",
        "    train_state['epoch_in'] = epoch_index\n",
        "\n",
        "    dataset.set_split('train')\n",
        "\n",
        "    batch_generator = generate_batches(\n",
        "        dataset= dataset, batch_size= args.batch_size, shuffle=True,\n",
        "        device = args.device, drop_last=False,\n",
        "        pinned_memory = True, n_workers = 3, \n",
        "    )\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    running_f1 = 0.0\n",
        "    model.train()\n",
        "\n",
        "    train_bar.reset(\n",
        "        total=dataset.get_num_batches(args.batch_size),\n",
        "    )\n",
        "\n",
        "    for batch_index, batch_dict in enumerate(batch_generator):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        loss,y_pred = model(\n",
        "            input_ids = batch_dict['x_data'],\n",
        "            attention_mask =  batch_dict['x_attn_mask'],\n",
        "            labels= batch_dict['y_target'].unsqueeze(1),\n",
        "        )[:2]\n",
        "        \n",
        "        y_pred = y_pred.view(-1, len(set(dataset.data_df.label)))\n",
        "                             \n",
        "#         scheduler.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "                             \n",
        "        loss_t = loss.item()\n",
        "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "                             \n",
        "        y_pred = y_pred.detach().cpu()\n",
        "        batch_dict['y_target'] = batch_dict['y_target'].cpu()\n",
        "        \n",
        "        acc_t = transformer_general_utils \\\n",
        "            .compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "        \n",
        "        f1_t = transformer_general_utils \\\n",
        "            .compute_macro_f1(y_pred, batch_dict['y_target'])\n",
        "\n",
        "        train_state['batch_preds'].append(y_pred)\n",
        "        train_state['batch_targets'].append(batch_dict['y_target'])\n",
        "        train_state['batch_indexes'].append(batch_dict['x_index'])\n",
        "\n",
        "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "        running_f1 += (f1_t - running_f1) / (batch_index + 1)\n",
        "\n",
        "        train_bar.set_postfix(loss = running_loss, f1 = running_f1, acc=running_acc,\n",
        "                             epoch=epoch_index)\n",
        "\n",
        "        train_bar.update()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    train_state['train_accuracies'].append(running_acc)\n",
        "    train_state['train_losses'].append(running_loss)\n",
        "    \n",
        "    train_state['train_preds'].append(\n",
        "        torch.cat(train_state['batch_preds']).cpu()\n",
        "    )\n",
        "    train_state['train_targets'].append(\n",
        "        torch.cat(train_state['batch_targets']).cpu()\n",
        "    )\n",
        "    train_state['train_indexes'].append(\n",
        "        torch.cat(train_state['batch_indexes']).cpu()\n",
        "    )\n",
        "    train_f1 = transformer_general_utils \\\n",
        "                .compute_macro_f1(train_state['train_preds'][-1],\n",
        "                                  train_state['train_targets'][-1],\n",
        "                                 )\n",
        "                                 \n",
        "    train_state['train_f1s'].append(train_f1)\n",
        "    \n",
        "    train_state['batch_preds'] = []\n",
        "    train_state['batch_targets'] = []\n",
        "    train_state['batch_indexes'] = []\n",
        "    \n",
        "    \n",
        "    dataset.set_split('val')\n",
        "    batch_generator = generate_batches(\n",
        "        dataset= dataset, batch_size= args.batch_size, shuffle=True,\n",
        "        device = args.device, drop_last=False,\n",
        "        pinned_memory = True, n_workers = 2, \n",
        "    )\n",
        "    eval_bar.reset(\n",
        "        total=dataset.get_num_batches(args.batch_size),\n",
        "    )\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    running_f1 = 0.0\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        optimizer._backup_and_load_cache()\n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\n",
        "            loss, y_pred = model(\n",
        "                input_ids = batch_dict['x_data'],\n",
        "                attention_mask =  batch_dict['x_attn_mask'],\n",
        "                labels= batch_dict['y_target'].unsqueeze(1),\n",
        "            )[:2]\n",
        "            y_pred = y_pred.view(-1, len(set(dataset.data_df.label)))\n",
        "            \n",
        "            loss_t = loss.item()\n",
        "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "            y_pred = y_pred.detach()\n",
        "            batch_dict['y_target'] = batch_dict['y_target'].cpu()\n",
        "            \n",
        "            acc_t = transformer_general_utils\\\n",
        "                .compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "            f1_t = transformer_general_utils \\\n",
        "                .compute_macro_f1(y_pred, batch_dict['y_target'])\n",
        "\n",
        "            train_state['batch_preds'].append(y_pred.cpu())\n",
        "            train_state['batch_targets'].append(batch_dict['y_target'].cpu())\n",
        "            train_state['batch_indexes'].append(batch_dict['x_index'].cpu())\n",
        "\n",
        "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "            running_f1 += (f1_t - running_f1) / (batch_index + 1)\n",
        "            \n",
        "\n",
        "            eval_bar.set_postfix(loss = running_loss, f1 = running_f1, acc=running_acc,\n",
        "                                 epoch=epoch_index)\n",
        "            eval_bar.update()\n",
        "            \n",
        "    train_state['val_accuracies'].append(running_acc)\n",
        "    train_state['val_losses'].append(running_loss)\n",
        "    \n",
        "        \n",
        "    train_state['val_preds'].append(\n",
        "        torch.cat(train_state['batch_preds']).cpu()\n",
        "    )\n",
        "\n",
        "    train_state['val_targets'].append(\n",
        "        torch.cat(train_state['batch_targets']).cpu()\n",
        "    )\n",
        "    train_state['val_indexes'].append(\n",
        "        torch.cat(train_state['batch_indexes']).cpu()\n",
        "    )\n",
        "    val_f1 = transformer_general_utils \\\n",
        "                .compute_macro_f1(train_state['val_preds'][-1],\n",
        "                                  train_state['val_targets'][-1],\n",
        "                                 )\n",
        "                                 \n",
        "    train_state['val_f1s'].append(val_f1)\n",
        "    \n",
        "    train_state['batch_preds'] = []\n",
        "    train_state['batch_targets'] = []\n",
        "    train_state['batch_indexes'] = []\n",
        "    \n",
        "    torch.save(\n",
        "        {\n",
        "            'model':model.state_dict(),\n",
        "        },\n",
        "        args.directory + f'_epoc_{epoch_index}_' + args.model_name,\n",
        "    )\n",
        "    \n",
        "    scheduler.step(val_f1)\n",
        "    early_stopping(val_f1, model)\n",
        "    optimizer._clear_and_load_backup()\n",
        "    epoch_bar.set_postfix( best_f1 = early_stopping.best_score, current = val_f1)\n",
        "    epoch_bar.update()    \n",
        "    \n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping\")\n",
        "        break\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvhJbjv3ouqM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epoch_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "UZ_tiTQsouqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_state['val_f1s'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGTvvqJOouqS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmsdTS5XouqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print('Train:',classification_report(\n",
        "    y_pred=(torch.argmax(train_state['train_preds'][-1],dim=1) ).cpu().long().numpy(),\n",
        "    y_true= train_state['train_targets'][-1].cpu().numpy(), \n",
        "    digits=4)\n",
        ")\n",
        "print('Dev:',classification_report(\n",
        "    y_pred=(torch.argmax(train_state['val_preds'][-1],dim=1) ).cpu().long().numpy(),\n",
        "    y_true= train_state['val_targets'][-1].cpu().numpy(), \n",
        "    digits=4)\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBvhO_3MouqX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_run_index = train_state['val_f1s'].index(max(train_state['val_f1s']))\n",
        "print('Train:',classification_report(\n",
        "    y_pred=(torch.argmax(train_state['train_preds'][best_run_index],dim=1) ).cpu().long().numpy(),\n",
        "    y_true= train_state['train_targets'][best_run_index].cpu().numpy(), \n",
        "    digits=4)\n",
        ")\n",
        "print('Dev:',classification_report(\n",
        "    y_pred=(torch.argmax(train_state['val_preds'][best_run_index],dim=1) ).cpu().long().numpy(),\n",
        "    y_true= train_state['val_targets'][best_run_index].cpu().numpy(), \n",
        "    digits=4)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyJlr1Ucouqa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sort_preds(indexes, preds):\n",
        "    \"\"\"Sorts the predictions in order, to reverse the effects of shuffle\n",
        "    done by dataloader\"\"\"\n",
        "    indexes = indexes.cpu().numpy().reshape(-1,1)\n",
        "    preds = preds.cpu().numpy()\n",
        "    arr_concat = np.hstack((indexes,preds)) #concat the preds and their indexes\n",
        "    sort_arr = arr_concat[ arr_concat[:,0].argsort()] #sort based on the indexes\n",
        "    sorted_preds = np.delete(sort_arr,0,axis=1)\n",
        "    return sorted_preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEKLcktCourg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_optimal_models_v2(train_state, split):\n",
        "    l = zip(train_state[f'{split}_f1s'], range(len(train_state[f'{split}_f1s'])))\n",
        "    sorted_vals = sorted(l, key = lambda x:x[0], reverse=True)\n",
        "    model_idxes = [i[1] for i in sorted_vals]\n",
        "    \n",
        "    trgts= sort_preds(train_state[f'{split}_indexes'][-1],train_state[f'{split}_targets'][-1].reshape(-1,1))\n",
        "    total_preds = len(train_state[f'{split}_indexes'])\n",
        "    init = np.zeros(train_state[f'{split}_preds'][-1].shape)\n",
        "    max_f1 = 0\n",
        "    idxes = []\n",
        "    for i in model_idxes:\n",
        "        temp = sort_preds(train_state[f'{split}_indexes'][i],train_state[f'{split}_preds'][i])\n",
        "        temp2 = init+temp\n",
        "        f1 = f1_score(\n",
        "            y_pred=temp2.argmax(axis=1),\n",
        "            y_true= trgts, average ='macro'\n",
        "        )\n",
        "        if f1 > max_f1:\n",
        "            max_f1 = f1\n",
        "            init = init+temp\n",
        "            idxes.append(i)\n",
        "    print(f'Taking preds from {idxes} | Dev f1:{f1}')\n",
        "    return idxes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "RI0eIVAtourj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_optimal_models = get_optimal_models_v2(train_state, 'val')\n",
        "final_optimal_models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siZdY82mours",
        "colab_type": "text"
      },
      "source": [
        "# Making preds on the given test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zcztc0lGourz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df = data_df_task_c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hkU-POXour5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataset = dataset\n",
        "test_dataset.set_split('test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iaq8vD0Xour7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataset._target_df.sample(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2PQOwWzousA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(test_df))\n",
        "print(test_dataset._target_df.split.value_counts())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRfZ2GLHousC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_testset(model, state, dataset, split,args):\n",
        "    \"\"\"Returns the final layer output of our transformer model\n",
        "    Puts them in the '{split}_*' keys in the state dict\n",
        "    Args:\n",
        "        model: A pytorch transformers model\n",
        "        state: dict to store outputs\n",
        "        dataset: A pytorch Dataset\n",
        "        split: The split on which to evaluate the model on\n",
        "        args: Arguments from namespace, etc\n",
        "    Returns:\n",
        "        state: all evaluated output stored in the \"test\" key\n",
        "    \"\"\"\n",
        "    eval_bar = notebook.tqdm(\n",
        "        desc = 'evaluation progress: ',\n",
        "        total=dataset.get_num_batches(args.batch_size),\n",
        "        position=0,\n",
        "        leave=False,\n",
        "    )\n",
        "    dataset.set_split(split)\n",
        "    batch_generator = generate_batches(\n",
        "        dataset= dataset, batch_size= args.batch_size, shuffle=False,\n",
        "        device = args.device, drop_last=False,\n",
        "        pinned_memory = True, n_workers = 2, \n",
        "    )\n",
        "    eval_bar.reset(\n",
        "        total=dataset.get_num_batches(args.batch_size),\n",
        "    )\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\n",
        "            y_pred = model(\n",
        "                input_ids = batch_dict['x_data'],\n",
        "                attention_mask =  batch_dict['x_attn_mask'],\n",
        "            )[0]\n",
        "            y_pred = y_pred.view(-1, 3)\n",
        "\n",
        "            y_pred = y_pred.detach()\n",
        "            \n",
        "            state['batch_preds'].append(y_pred.cpu())\n",
        "            state['batch_indexes'].append(batch_dict['x_index'].cpu())\n",
        "            \n",
        "            eval_bar.update()\n",
        "            \n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "    \n",
        "    state[f'{split}_preds'].append(\n",
        "        torch.cat(state['batch_preds']).cpu()\n",
        "    )\n",
        "    state[f'{split}_indexes'].append(\n",
        "        torch.cat(state['batch_indexes']).cpu()\n",
        "    )\n",
        "    \n",
        "    state['batch_preds'] = []\n",
        "    state['batch_indexes'] = []\n",
        "    \n",
        "    eval_bar.close()\n",
        "    return state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDLAcVoOousD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chosen_models = [all_model_paths[i] for i in final_optimal_models]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRhTG0jJousG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_state = general_utils.make_train_state()\n",
        "for model_path in notebook.tqdm(chosen_models, total=len(chosen_models)):\n",
        "    model.load_state_dict(torch.load(model_path)['model'])\n",
        "    test_state = evaluate_testset(model, test_state, test_dataset, 'test',args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyPLbSx_ousH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_state['test_preds'][-1].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9zIQn2PousJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[test_state['test_preds'][i].size() for i in range(len(test_state['test_preds']))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9k7efmwXousM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(test_dataset._target_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-mAcbFFousO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.zeros_like(test_state['test_preds'][0]).size()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zw23sdIAousQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ensemble_pred = torch.zeros_like(test_state['test_preds'][0])\n",
        "for i in test_state['test_preds']:\n",
        "    ensemble_pred += i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1tEtoDvousS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# label_dict[\"IND\"] = 0\n",
        "# label_dict[\"GRP\"] = 1\n",
        "# label_dict[\"OTH\"] = 2\n",
        "#ref utils/offeval2020.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7l7scgnousU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "int_to_label = { 0: 'IND', 1:'GRP', 2:'OTH'}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIG3GtyDousW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = []\n",
        "for i in torch.argmax(ensemble_pred, dim=1):\n",
        "    t.append(int_to_label[i.item()])\n",
        "\n",
        "collections.Counter(t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N43KI4P4ousY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert len(t) == len(test_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANkfSvUSousa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "offeval_task_c_pred_analysis_df = pd.DataFrame(\n",
        "    data={\n",
        "        'id':test_df.id,\n",
        "        'text':test_df.tweet,\n",
        "        'label':t,\n",
        "    }\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tYSn6VNousb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "offeval_task_c_pred_label_df = pd.DataFrame(\n",
        "    data={\n",
        "        'id':test_df.id,\n",
        "        'label':t,\n",
        "    }\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ompoxc6Nousc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "offeval_task_c_pred_analysis_df.to_csv(\n",
        "    'offeval_task_c_pred_analysis.csv',index=False,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHiGB2Q-ouse",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "offeval_task_c_pred_label_df.to_csv(\n",
        "    'offeval_task_c_pred_label.csv', index=False, header=False,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IVzPSmwousf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "offeval_task_c_pred_label_df.label.value_counts()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mutV5hWkoush",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "offeval_task_c_pred_label_df.label.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr707IBvousi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}