{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "Eng Task A - Ensemble DistilGPT2.ipynb",
      "provenance": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6feeac9b446f4e45833dcc5cf7d3cede": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7b69eef6cd674a378fb5fdc11ddb3e4d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a51cce13a58c442db411d6647ba12657",
              "IPY_MODEL_d1116c7d8ec84be3a5cb700dbe7fd3c0"
            ]
          }
        },
        "7b69eef6cd674a378fb5fdc11ddb3e4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a51cce13a58c442db411d6647ba12657": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0cb5f61b6e874104b5d348c477e8324a",
            "_dom_classes": [],
            "description": "training_routine:   0%",
            "_model_name": "IntProgressModel",
            "bar_style": "",
            "max": 4,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7e5ae51973834f5abc83ae32fbaf77dc"
          }
        },
        "d1116c7d8ec84be3a5cb700dbe7fd3c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f4bfb5951e43474c938b1bca293a8151",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/4 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6303474cc077463a91bcbbc5aedae360"
          }
        },
        "0cb5f61b6e874104b5d348c477e8324a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7e5ae51973834f5abc83ae32fbaf77dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f4bfb5951e43474c938b1bca293a8151": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6303474cc077463a91bcbbc5aedae360": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c89974d9cbf745e6b6f83fc9cba0b5e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f5dc86f6a88b412aaef9821bd11563e1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7c038533614448469d8250cf48caf6b7",
              "IPY_MODEL_d3b2c5b0ff894da28154ca2c07c8b4ce"
            ]
          }
        },
        "f5dc86f6a88b412aaef9821bd11563e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7c038533614448469d8250cf48caf6b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_401648b60a354c7da2e7d7217db9e8e1",
            "_dom_classes": [],
            "description": "split=train :   7%",
            "_model_name": "IntProgressModel",
            "bar_style": "",
            "max": 7952,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 564,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b2f930b20acd4cdba48c1475689fb36e"
          }
        },
        "d3b2c5b0ff894da28154ca2c07c8b4ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e6ce22370715421b9152013d889a490a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 564/7952 [04:39&lt;1:00:54,  2.02it/s, acc=0.775, epoch=0, f1=0.548, loss=1.37]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4a3658cd66f94d7fba620a5c4c213c4d"
          }
        },
        "401648b60a354c7da2e7d7217db9e8e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b2f930b20acd4cdba48c1475689fb36e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e6ce22370715421b9152013d889a490a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4a3658cd66f94d7fba620a5c4c213c4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e7c12eb885b54f36ba80204e2d823c94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_02499aa163304b33bd4f5063da53f9c1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8f689af6d46f42d48c3dfa28988f5e1e",
              "IPY_MODEL_d703af78902543268eef972e80b167b7"
            ]
          }
        },
        "02499aa163304b33bd4f5063da53f9c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8f689af6d46f42d48c3dfa28988f5e1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1e03ddd8e3fb499fb169669638269237",
            "_dom_classes": [],
            "description": "split=eval:   0%",
            "_model_name": "IntProgressModel",
            "bar_style": "",
            "max": 1704,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_22b2bc157d8742f3999f595aee813ede"
          }
        },
        "d703af78902543268eef972e80b167b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bbca8f443d624de6abd15509e2fa5cad",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/1704 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c59a77eee0a742ee87adcdffec9b7c6c"
          }
        },
        "1e03ddd8e3fb499fb169669638269237": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "22b2bc157d8742f3999f595aee813ede": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bbca8f443d624de6abd15509e2fa5cad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c59a77eee0a742ee87adcdffec9b7c6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaHXdQweG2gY",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cozek/OffensEval2020-code/blob/master/notebooks/Eng%20Task%20A%20-%20Ensemble%20DistilGPT2.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnXQrkMlG2ga",
        "colab_type": "text"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UcEUgF4G2gc",
        "colab_type": "text"
      },
      "source": [
        "At the time of our work, we used the following library versions\n",
        "- numpy 1.18.1\n",
        "- pandas 1.0.1\n",
        "- torch 1.2.0\n",
        "- Cuda 10.0\n",
        "- python 3.7.0\n",
        "- sklearn 0.22.1\n",
        "- tqdm 4.42.1\n",
        "- nltk 3.4.5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jknbt-9FG9sq",
        "colab_type": "code",
        "outputId": "db3a613d-1777-46ac-e4e9-d33c99dac948",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "!git clone https://github.com/cozek/OffensEval2020-code/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'OffensEval2020-code' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt7C7pQyHPLp",
        "colab_type": "code",
        "outputId": "aae78351-e261-439f-ce84-50a369add529",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        }
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers\n",
        "!pip install /content/transformers/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'transformers' already exists and is not an empty directory.\n",
            "Processing ./transformers\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.18.3)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (4.38.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.1.86)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.0.43)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (0.14.1)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-2.8.0-cp36-none-any.whl size=595751 sha256=0c0b076b0a5466113c502fbe338e369a3440a032a037898d5476be2aada9a8a7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-36kz5t9v/wheels/23/19/dd/2561a4e47240cf6b307729d58e56f8077dd0c698f5992216cf\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Found existing installation: transformers 2.8.0\n",
            "    Uninstalling transformers-2.8.0:\n",
            "      Successfully uninstalled transformers-2.8.0\n",
            "Successfully installed transformers-2.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmMKCJ-dG2ge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/OffensEval2020-code/src/')\n",
        "import collections\n",
        "from typing import Callable\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "import pandas as pd\n",
        "from tqdm import notebook\n",
        "import importlib\n",
        "import pprint\n",
        "import nltk\n",
        "import datetime\n",
        "import os\n",
        "from argparse import Namespace\n",
        "import re\n",
        "from collections import Counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWwTBxXqG2gh",
        "colab_type": "code",
        "outputId": "b04d9738-70ce-4089-8430-73e89f161c49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        }
      },
      "source": [
        "import utils.general as general_utils\n",
        "import utils.trac2020 as trac_utils\n",
        "import utils.transformer.data as transformer_data_utils\n",
        "import utils.transformer.general as transformer_general_utils\n",
        "general_utils.set_seed_everywhere() #set the seed for reproducibility"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EfdINgcG2gk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(level=logging.INFO) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "6k1nQ5d4G2gn",
        "colab_type": "code",
        "outputId": "9c6fdaf5-3387-4924-c755-ff9983d80658",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "torch.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.5.0+cu101'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcFCAJAQG2gr",
        "colab_type": "text"
      },
      "source": [
        "## Import Optimzer and XLM Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc66UeiyG2gs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import RAdam and Lookahead\n",
        "from radam.radam import RAdam\n",
        "from lookahead.optimizer import Lookahead"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "sF7KP8tzG2gv",
        "colab_type": "code",
        "outputId": "4f84764c-9955-4db4-c823-793135b08c54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        }
      },
      "source": [
        "from transformers import GPT2Tokenizer, GPT2Model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.file_utils:PyTorch version 1.5.0+cu101 available.\n",
            "INFO:transformers.file_utils:TensorFlow version 2.2.0-rc3 available.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIAYRiqhG2gz",
        "colab_type": "text"
      },
      "source": [
        "# Set up the argspace/important_variables\n",
        "Please note that performance is suseptible to hyper parameters. We used a Nvidia Tesla V100 32GB. If you lower the batch size or change any other parameters, modules to fit your machine, you might not get the same performance as reported in our paper.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F6tx_VrG2gz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = Namespace(\n",
        "        #use cuda by default\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    \n",
        "        #set batch size and number of epochs\n",
        "        batch_size = 32,\n",
        "        num_epochs = 20,\n",
        "    \n",
        "        #set the learning rate\n",
        "        learning_rate = 0.0001,\n",
        "\n",
        "        #location of the train, dev and test csv\n",
        "        train_val_csv = '/content/OffensEval2020-code/data/eng/task_a_tiny.zip',\n",
        "        test_csv = '/content/OffensEval2020-code/data/test_data/test_a_tweets.tsv',\n",
        "    \n",
        "        #directory to save our models at\n",
        "        directory = './', \n",
        "        model_name = 'distilGPT2.pt',\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HY29sw-7G2g2",
        "colab_type": "text"
      },
      "source": [
        "## Loading a presplit subset of the full dataset data into DataFrames. \n",
        "Here, 0 : NOT, 1 : HOF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKKJ-JmZG2g3",
        "colab_type": "code",
        "outputId": "39bac445-83cb-4630-e186-9c1a8108e753",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "data_df =  pd.read_csv(args.train_val_csv, compression = 'zip')\n",
        "data_df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>average</th>\n",
              "      <th>std</th>\n",
              "      <th>label</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1159552003770650625</td>\n",
              "      <td>@USER dancing is a talent show it offffff like...</td>\n",
              "      <td>0.321070</td>\n",
              "      <td>0.235700</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1159542583410024449</td>\n",
              "      <td>Cooking Tweeps!  I am desirous of a new ridged...</td>\n",
              "      <td>0.153384</td>\n",
              "      <td>0.168776</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1159576849976107009</td>\n",
              "      <td>@USER I can respect that I’ll see I kind of li...</td>\n",
              "      <td>0.161124</td>\n",
              "      <td>0.200444</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1159661814160011264</td>\n",
              "      <td>That song used to bring tears to my eyes in 5t...</td>\n",
              "      <td>0.316731</td>\n",
              "      <td>0.168065</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1159620953686495233</td>\n",
              "      <td>Nah.  Ranked. 1. Abortion 2. Guns 3.  Religiou...</td>\n",
              "      <td>0.428485</td>\n",
              "      <td>0.064754</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90885</th>\n",
              "      <td>1159536832469774340</td>\n",
              "      <td>This bitch wants people to die in a Mass Murder!</td>\n",
              "      <td>0.860484</td>\n",
              "      <td>0.172114</td>\n",
              "      <td>1</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90886</th>\n",
              "      <td>1159571431115698176</td>\n",
              "      <td>rly feel like the ugliest girl in the world im...</td>\n",
              "      <td>0.593529</td>\n",
              "      <td>0.200248</td>\n",
              "      <td>1</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90887</th>\n",
              "      <td>1159547322545725440</td>\n",
              "      <td>@USER I have the ss and was defending her cuz ...</td>\n",
              "      <td>0.790050</td>\n",
              "      <td>0.136956</td>\n",
              "      <td>1</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90888</th>\n",
              "      <td>1159662323260362752</td>\n",
              "      <td>I THOUGHT SHE WAS PREGNANT BUT THAT WAS HER AS...</td>\n",
              "      <td>0.696626</td>\n",
              "      <td>0.235508</td>\n",
              "      <td>1</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90889</th>\n",
              "      <td>1159539567361347584</td>\n",
              "      <td>@USER Who the hell are you?Have you forgotten ...</td>\n",
              "      <td>0.828747</td>\n",
              "      <td>0.169144</td>\n",
              "      <td>1</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>90890 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                        id  ...  split\n",
              "0      1159552003770650625  ...  train\n",
              "1      1159542583410024449  ...  train\n",
              "2      1159576849976107009  ...  train\n",
              "3      1159661814160011264  ...  train\n",
              "4      1159620953686495233  ...  train\n",
              "...                    ...  ...    ...\n",
              "90885  1159536832469774340  ...   test\n",
              "90886  1159571431115698176  ...   test\n",
              "90887  1159547322545725440  ...   test\n",
              "90888  1159662323260362752  ...   test\n",
              "90889  1159539567361347584  ...   test\n",
              "\n",
              "[90890 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0ZZu7WmKCxK",
        "colab_type": "code",
        "outputId": "9715bfe4-303b-47a2-d427-15fc61d12f65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        }
      },
      "source": [
        "#0 = NOT, 1 = HOF\n",
        "data_df.label.value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    76402\n",
              "1    14488\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCdT3E0zKdPy",
        "colab_type": "code",
        "outputId": "7fd57bf4-b5ca-4457-9b00-ff739a598d39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        }
      },
      "source": [
        "with pd.option_context('display.max_colwidth', -1): \n",
        "    print(data_df[['text','label']].sample(5))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                                                                                           text  label\n",
            "31725  @USER BRO I WENT TODAY AND LITERALLY RAN INTO HOT TOPIC TO FIND UR SHIRT ANd i LITERALLY ALMOST CRIED I WANTED IT SO BAD  0    \n",
            "65508  Always I’ve existed in a state of love. I know of no other way. It’d be an alien tongue, even if I tried to claim it.     0    \n",
            "42246  @USER @USER Thank you and Nancy but Trump doesn't read and will do nothing.                                               0    \n",
            "14712  Blink 182 at the Forum ☺️                                                                                                 0    \n",
            "90625  @USER Why lower the flag to begin with if he's such a racist? Utter stupidity                                             1    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIwzFVj6KT37",
        "colab_type": "code",
        "outputId": "774637f9-9179-459f-b8f0-ff21be4669eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "data_df.split.value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "train    63622\n",
              "test     13635\n",
              "val      13633\n",
              "Name: split, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_g10Czw0KJth",
        "colab_type": "code",
        "outputId": "2d433cc5-2c0d-43b6-e8b1-db714e290dab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "len(data_df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "90890"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKy-82YrG2hh",
        "colab_type": "text"
      },
      "source": [
        "## Create the text preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyhSisD5G2hi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GPT2Preprocessor:\n",
        "    def __init__(self, transformer_tokenizer, sentence_detector):\n",
        "        self.transformer_tokenizer = transformer_tokenizer\n",
        "        self.sentence_detector = sentence_detector\n",
        "\n",
        "    def add_eos_tokens(self, text):\n",
        "        eos_token = \" \" + self.transformer_tokenizer.eos_token + \" \"\n",
        "        sentences = self.sentence_detector.tokenize(text)\n",
        "        eos_added_text = (\n",
        "            eos_token.join(sentences) + \" \" + self.transformer_tokenizer.eos_token\n",
        "        )\n",
        "        return eos_added_text\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGCMVmYNJHca",
        "colab_type": "code",
        "outputId": "762cb166-0b4f-433a-9e93-774f71b7d8d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        }
      },
      "source": [
        "!python -c 'import nltk; nltk.download(\"punkt\")'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "uX6GH1WNG2hl",
        "colab_type": "code",
        "outputId": "891b235d-9efd-4cb6-a8ab-4807a931b3f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
        "punkt_sentence_detector = nltk.data.load('tokenizers/punkt/english.pickle')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/distilgpt2-vocab.json from cache at /root/.cache/torch/transformers/71cc2431cf0b5bbe7a23601a808ed322c90251c8261b46f04970140a3c2c1cb4.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/distilgpt2-merges.txt from cache at /root/.cache/torch/transformers/4faf7afb02a1ea7d2944e9ba7a175c7b8de4957cdbae75cd5ddffc7c7643ebbc.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XZCJQllG2hn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2_preproc = GPT2Preprocessor(gpt2_tokenizer, punkt_sentence_detector)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U32yNFVz9co_",
        "colab_type": "code",
        "outputId": "7cb6581e-9bc3-44a5-b121-a643c9713ca7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "source": [
        "data_df['text']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        @USER dancing is a talent show it offffff like...\n",
              "1        Cooking Tweeps!  I am desirous of a new ridged...\n",
              "2        @USER I can respect that I’ll see I kind of li...\n",
              "3        That song used to bring tears to my eyes in 5t...\n",
              "4        Nah.  Ranked. 1. Abortion 2. Guns 3.  Religiou...\n",
              "                               ...                        \n",
              "90885     This bitch wants people to die in a Mass Murder!\n",
              "90886    rly feel like the ugliest girl in the world im...\n",
              "90887    @USER I have the ss and was defending her cuz ...\n",
              "90888    I THOUGHT SHE WAS PREGNANT BUT THAT WAS HER AS...\n",
              "90889    @USER Who the hell are you?Have you forgotten ...\n",
              "Name: text, Length: 90890, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4JACIdi9bWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#add the special tokens\n",
        "data_df[\"text\"] = data_df[\"text\"].map(gpt2_preproc.add_eos_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43q7GSYEG2hu",
        "colab_type": "text"
      },
      "source": [
        "### Create the torch torch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlI6CoLfG2hv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = transformer_data_utils.HateDataset(\n",
        "        data_df=data_df, tokenizer=gpt2_tokenizer\n",
        "    )\n",
        "assert dataset._max_seq_length <= 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0izVXm7xG2h2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batches(dataset, batch_size, shuffle=True,\n",
        "                     drop_last=False, device=\"cpu\", pinned_memory = False, n_workers = 0): \n",
        "    \"\"\"\n",
        "    A generator function which wraps the PyTorch DataLoader. It will \n",
        "      ensure each tensor is on the write device location.\n",
        "    \"\"\"\n",
        "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
        "                            shuffle=shuffle, drop_last=drop_last,\n",
        "                            pin_memory= pinned_memory,\n",
        "                            num_workers = n_workers,\n",
        "                            )\n",
        "    \n",
        "    for data_dict in dataloader:\n",
        "        out_data_dict = {}\n",
        "        out_data_dict['x_data'] = data_dict['x_data'].to(\n",
        "            device, non_blocking= (True if pinned_memory else False) \n",
        "        )\n",
        "        out_data_dict['x_attn_mask'] = data_dict['x_attn_mask'].to(\n",
        "            device, non_blocking= (True if pinned_memory else False) \n",
        "        )\n",
        "        out_data_dict['x_index'] = data_dict['x_index']\n",
        "        out_data_dict['y_target'] = data_dict['y_target'].to(\n",
        "            device, non_blocking= (True if pinned_memory else False) \n",
        "        )\n",
        "        yield out_data_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZsYMvrI_da7",
        "colab_type": "text"
      },
      "source": [
        "# Creating the Classifier "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYFlt6Au_F1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleGPT2SequenceClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        hidden_size: int,\n",
        "        num_classes:int ,\n",
        "        max_seq_len:int,\n",
        "        gpt_model_name:str, \n",
        "    ):\n",
        "        super(SimpleGPT2SequenceClassifier,self).__init__()\n",
        "        self.gpt2model = GPT2Model.from_pretrained(\n",
        "            gpt_model_name\n",
        "        )\n",
        "        self.fc1 = nn.Linear(hidden_size, num_classes)\n",
        "        \n",
        "    def forward(self, x_in):\n",
        "        \n",
        "        gpt_out = self.gpt2model(x_in)[0] #returns tuple\n",
        "        batch_size = gpt_out.shape[0]\n",
        "        prediction_vector = self.fc1(gpt_out.view(batch_size,-1)) #(batch_size , max_len, num_classes)\n",
        "    \n",
        "        return prediction_vector\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4MGsBSm_oXR",
        "colab_type": "code",
        "outputId": "4ac83bf2-8f24-4b08-be1d-06c4fd9ba896",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        }
      },
      "source": [
        "print(\"Loading Pretrained distilgpt2...\")\n",
        "num_classes = len(set(data_df.label))\n",
        "hidden_size = dataset._max_seq_length * 768\n",
        "model = SimpleGPT2SequenceClassifier(\n",
        "    hidden_size=hidden_size,\n",
        "    num_classes=num_classes,\n",
        "    gpt_model_name=\"distilgpt2\",\n",
        "    max_seq_len=dataset._max_seq_length,\n",
        ")\n",
        "model.to(args.device)\n",
        "print(\"Finished\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Pretrained distilgpt2...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilgpt2-config.json from cache at /root/.cache/torch/transformers/eb0f77b3f095880586731f57e2fe19060d71d1036ef8daf727bd97a17fb66a43.a8b35e282ef6b386ae29500ff942def3dd5a8bf71de78a3d36221d6d90031bb5\n",
            "INFO:transformers.configuration_utils:Model config GPT2Config {\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/distilgpt2-pytorch_model.bin from cache at /root/.cache/torch/transformers/cd250f30004d0dee11ff1af311bd3facb6f38739fd870b9c8aa9321333a550be.ffe4c53a2a410b15148cf4170cc408d2d2f98adeecdde146ef8e71843039ff3c\n",
            "INFO:transformers.modeling_utils:Weights of GPT2Model not initialized from pretrained model: ['transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.masked_bias']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLAYZmmtG2iJ",
        "colab_type": "text"
      },
      "source": [
        "# Begin Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtX_A6l0Aq6r",
        "colab_type": "code",
        "outputId": "a756777e-e865-4905-ecab-c38a41ce3d27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "loss_func = nn.CrossEntropyLoss()\n",
        "base_optimizer = RAdam(model.parameters(), lr=args.learning_rate)\n",
        "optimizer = Lookahead(optimizer=base_optimizer, k=5, alpha=0.5)\n",
        "args.num_epochs = 4\n",
        "print(f\"Using LR:{args.learning_rate}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using LR:0.0001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "vC3ewUSnG2iK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_state = general_utils.make_train_state()\n",
        "train_state[\"ckpt\"] = 0\n",
        "train_state['max_seq_len'] = dataset._max_seq_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lr0zwdnYKNtf",
        "colab_type": "code",
        "outputId": "38ba8240-2885-4141-921e-3c9c71d6b6b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon May  4 05:19:23 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    22W /  75W |   1299MiB /  7611MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc81HVEZKWzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args.batch_size = 14"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "qg3U6LcmG2iN",
        "colab_type": "code",
        "outputId": "81503aed-3ecc-42e2-b307-e39a2113f05d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387,
          "referenced_widgets": [
            "6feeac9b446f4e45833dcc5cf7d3cede",
            "7b69eef6cd674a378fb5fdc11ddb3e4d",
            "a51cce13a58c442db411d6647ba12657",
            "d1116c7d8ec84be3a5cb700dbe7fd3c0",
            "0cb5f61b6e874104b5d348c477e8324a",
            "7e5ae51973834f5abc83ae32fbaf77dc",
            "f4bfb5951e43474c938b1bca293a8151",
            "6303474cc077463a91bcbbc5aedae360",
            "c89974d9cbf745e6b6f83fc9cba0b5e7",
            "f5dc86f6a88b412aaef9821bd11563e1",
            "7c038533614448469d8250cf48caf6b7",
            "d3b2c5b0ff894da28154ca2c07c8b4ce",
            "401648b60a354c7da2e7d7217db9e8e1",
            "b2f930b20acd4cdba48c1475689fb36e",
            "e6ce22370715421b9152013d889a490a",
            "4a3658cd66f94d7fba620a5c4c213c4d",
            "e7c12eb885b54f36ba80204e2d823c94",
            "02499aa163304b33bd4f5063da53f9c1",
            "8f689af6d46f42d48c3dfa28988f5e1e",
            "d703af78902543268eef972e80b167b7",
            "1e03ddd8e3fb499fb169669638269237",
            "22b2bc157d8742f3999f595aee813ede",
            "bbca8f443d624de6abd15509e2fa5cad",
            "c59a77eee0a742ee87adcdffec9b7c6c"
          ]
        }
      },
      "source": [
        "epoch_bar = notebook.tqdm(\n",
        "    desc = 'training_routine',\n",
        "    total = args.num_epochs,\n",
        "    position=0,\n",
        "    leave = True,\n",
        ")\n",
        "dataset.set_split('train')\n",
        "train_bar = notebook.tqdm(\n",
        "    desc = 'split=train ',\n",
        "    total=dataset.get_num_batches(args.batch_size),\n",
        "    position=0,\n",
        "    leave=True,\n",
        ")\n",
        "dataset.set_split('val')\n",
        "eval_bar = notebook.tqdm(\n",
        "    desc = 'split=eval',\n",
        "    total=dataset.get_num_batches(args.batch_size),\n",
        "    position=0,\n",
        "    leave=True,\n",
        ")\n",
        "\n",
        "for epoch_index in range(args.num_epochs):\n",
        "    train_state['epoch_in'] = epoch_index\n",
        "\n",
        "    dataset.set_split('train')\n",
        "    batch_generator = transformer_data_utils.generate_batches(\n",
        "        dataset=dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=True,\n",
        "        device=args.device,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    running_f1 = 0.0\n",
        "    model.train()\n",
        "\n",
        "    train_bar.reset(\n",
        "        total=dataset.get_num_batches(args.batch_size),\n",
        "    )\n",
        "    model.train()\n",
        "    for batch_index, batch_dict in enumerate(batch_generator):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        y_pred = model(batch_dict[\"x_data\"])\n",
        "\n",
        "        loss = loss_func(y_pred, batch_dict[\"y_target\"])\n",
        "        loss_t = loss.item()\n",
        "\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "                             \n",
        "        loss_t = loss.item()\n",
        "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "                             \n",
        "        y_pred = y_pred.detach().cpu()\n",
        "\n",
        "        batch_dict['y_target'] = batch_dict['y_target'].cpu()\n",
        "        \n",
        "        acc_t = transformer_general_utils.compute_accuracy(\n",
        "            y_pred, batch_dict[\"y_target\"]\n",
        "        )\n",
        "\n",
        "        f1_t = transformer_general_utils.compute_macro_f1(\n",
        "            y_pred, batch_dict[\"y_target\"]\n",
        "        )\n",
        "\n",
        "        train_state[\"batch_preds\"].append(y_pred)\n",
        "        train_state[\"batch_targets\"].append(batch_dict[\"y_target\"])\n",
        "        train_state[\"batch_indexes\"].append(batch_dict[\"x_index\"])\n",
        "\n",
        "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "        running_f1 += (f1_t - running_f1) / (batch_index + 1)\n",
        "\n",
        "        train_bar.set_postfix(loss = running_loss, f1 = running_f1, acc=running_acc,\n",
        "                             epoch=epoch_index)\n",
        "\n",
        "        train_bar.update()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    train_state['train_accuracies'].append(running_acc)\n",
        "    train_state['train_losses'].append(running_loss)\n",
        "    \n",
        "    train_state['train_preds'].append(\n",
        "        torch.cat(train_state['batch_preds']).cpu()\n",
        "    )\n",
        "    train_state['train_targets'].append(\n",
        "        torch.cat(train_state['batch_targets']).cpu()\n",
        "    )\n",
        "    train_state['train_indexes'].append(\n",
        "        torch.cat(train_state['batch_indexes']).cpu()\n",
        "    )\n",
        "    train_f1 = transformer_general_utils \\\n",
        "                .compute_macro_f1(train_state['train_preds'][-1],\n",
        "                                  train_state['train_targets'][-1],\n",
        "                                 )\n",
        "                                 \n",
        "    train_state['train_f1s'].append(train_f1)\n",
        "    \n",
        "    train_state['batch_preds'] = []\n",
        "    train_state['batch_targets'] = []\n",
        "    train_state['batch_indexes'] = []\n",
        "    \n",
        "    \n",
        "    dataset.set_split('val')\n",
        "    batch_generator = generate_batches(\n",
        "        dataset= dataset, batch_size= args.batch_size, shuffle=True,\n",
        "        device = args.device, drop_last=False,\n",
        "        pinned_memory = False, n_workers = 2, \n",
        "    )\n",
        "    eval_bar.reset(\n",
        "        total=dataset.get_num_batches(args.batch_size),\n",
        "    )\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    running_f1 = 0.0\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        optimizer._backup_and_load_cache()\n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\n",
        "            y_pred = model(batch_dict[\"x_data\"])\n",
        "\n",
        "            loss = loss_func(y_pred, batch_dict[\"y_target\"])\n",
        "            loss_t = loss.item()\n",
        "\n",
        "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "            y_pred = y_pred.detach()\n",
        "            \n",
        "            acc_t = transformer_general_utils.compute_accuracy(\n",
        "                y_pred, batch_dict[\"y_target\"]\n",
        "            )\n",
        "\n",
        "            f1_t = transformer_general_utils.compute_macro_f1(\n",
        "                y_pred, batch_dict[\"y_target\"]\n",
        "            )\n",
        "\n",
        "            train_state['batch_preds'].append(y_pred.cpu())\n",
        "            train_state['batch_targets'].append(batch_dict['y_target'])\n",
        "            train_state['batch_indexes'].append(batch_dict['x_index'].cpu())\n",
        "\n",
        "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "            running_f1 += (f1_t - running_f1) / (batch_index + 1)\n",
        "            \n",
        "\n",
        "            eval_bar.set_postfix(loss = running_loss, f1 = running_f1, acc=running_acc,\n",
        "                                 epoch=epoch_index)\n",
        "            eval_bar.update()\n",
        "            \n",
        "    train_state['val_accuracies'].append(running_acc)\n",
        "    train_state['val_losses'].append(running_loss)\n",
        "    \n",
        "        \n",
        "    train_state['val_preds'].append(\n",
        "        torch.cat(train_state['batch_preds']).cpu()\n",
        "    )\n",
        "\n",
        "    train_state['val_targets'].append(\n",
        "        torch.cat(train_state['batch_targets']).cpu()\n",
        "    )\n",
        "\n",
        "    train_state['val_indexes'].append(\n",
        "        torch.cat(train_state['batch_indexes']).cpu()\n",
        "    )\n",
        "\n",
        "    val_f1 = transformer_general_utils \\\n",
        "                .compute_macro_f1(train_state['val_preds'][-1],\n",
        "                                  train_state['val_targets'][-1],\n",
        "                                 )\n",
        "          \n",
        "    train_state['val_f1s'].append(val_f1)\n",
        "    \n",
        "    train_state['batch_preds'] = []\n",
        "    train_state['batch_targets'] = []\n",
        "    train_state['batch_indexes'] = []\n",
        "    \n",
        "    torch.save(\n",
        "        {\n",
        "            'model':model.state_dict(),\n",
        "        },\n",
        "        args.directory + f'_epoc_{epoch_index}_' + args.model_name,\n",
        "    )\n",
        "    \n",
        "    optimizer._clear_and_load_backup()\n",
        "    epoch_bar.set_postfix(best_f1 = early_stopping.best_score, current = val_f1)\n",
        "    epoch_bar.update()\n",
        "\n",
        "    epoch_bar.set_postfix(best_f1 = early_stopping.best_score, current = val_f1)\n",
        "    epoch_bar.update()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6feeac9b446f4e45833dcc5cf7d3cede",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='training_routine', max=4, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c89974d9cbf745e6b6f83fc9cba0b5e7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='split=train ', max=7952, style=ProgressStyle(description_widt…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7c12eb885b54f36ba80204e2d823c94",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='split=eval', max=1704, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of addcmul_ is deprecated:\n",
            "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
            "Consider using one of the following signatures instead:\n",
            "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-c66fca736a1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"y_target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mloss_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "X3cbb-1dG2iQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_state['train_f1s'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoYjyEe1G2iS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_state['val_f1s'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJePw9boG2iU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrQhfw2iG2iY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_run_index = train_state['val_f1s'].index(max(train_state['val_f1s']))\n",
        "print(f'Best run at epoch {best_run_index}')\n",
        "print('Train:',classification_report(\n",
        "    y_pred=(torch.argmax(train_state['train_preds'][best_run_index],dim=1) ).cpu().long().numpy(),\n",
        "    y_true= train_state['train_targets'][best_run_index].cpu().numpy(), \n",
        "    digits=4)\n",
        ")\n",
        "print('Dev:',classification_report(\n",
        "    y_pred=(torch.argmax(train_state['val_preds'][best_run_index],dim=1) ).cpu().long().numpy(),\n",
        "    y_true= train_state['val_targets'][best_run_index].cpu().numpy(), \n",
        "    digits=4)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI1XaxsPG2ib",
        "colab_type": "text"
      },
      "source": [
        "## Check if ensembling helps and pick models to use on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOEv36m1G2ib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sort_preds(indexes, preds):\n",
        "    \"\"\"Sorts the predictions in order, to reverse the effects of shuffle\n",
        "    done by dataloader\"\"\"\n",
        "    indexes = indexes.cpu().numpy().reshape(-1,1)\n",
        "    preds = preds.cpu().numpy()\n",
        "    arr_concat = np.hstack((indexes,preds)) #concat the preds and their indexes\n",
        "    sort_arr = arr_concat[ arr_concat[:,0].argsort()] #sort based on the indexes\n",
        "    sorted_preds = np.delete(sort_arr,0,axis=1)\n",
        "    return sorted_preds\n",
        "\n",
        "def get_optimal_models(train_state, split, reverse=False ):\n",
        "    \"\"\"Naive Ensembling\"\"\"\n",
        "    trgts= sort_preds(train_state[f'{split}_indexes'][-1],train_state[f'{split}_targets'][-1].reshape(-1,1))\n",
        "    total_preds = len(train_state[f'{split}_indexes'])\n",
        "    init = np.zeros(train_state[f'{split}_preds'][-1].shape)\n",
        "    max_f1 = 0\n",
        "    idxes = []\n",
        "    rng = range(0,total_preds)\n",
        "    if reverse:\n",
        "        rng = reversed(rng)\n",
        "    for i in rng:\n",
        "        temp = sort_preds(train_state[f'{split}_indexes'][i],train_state[f'{split}_preds'][i])\n",
        "        temp2 = init+temp\n",
        "        f1 = f1_score(\n",
        "            y_pred=temp2.argmax(axis=1),\n",
        "            y_true= trgts, average ='weighted'\n",
        "        )\n",
        "        if f1 > max_f1:\n",
        "            max_f1 = f1\n",
        "            init = init+temp\n",
        "            idxes.append(i)\n",
        "    print(f'Taking preds from {idxes} | Dev f1:{f1}')\n",
        "    return (idxes,max_f1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDBXq6bCX5P_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_state['val_f1s']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "vWIVkH1yG2ie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_model_f1_score = f1_score(\n",
        "    y_pred=(torch.argmax(train_state['val_preds'][best_run_index],dim=1) ).cpu().long().numpy(),\n",
        "    y_true= train_state['val_targets'][best_run_index].cpu().numpy(), \n",
        "    average='weighted'\n",
        ")\n",
        "_models= [get_optimal_models(train_state,'val', reverse=False),\n",
        "                 get_optimal_models(train_state,'val', reverse=True),\n",
        "                 ([best_run_index],best_model_f1_score),]\n",
        "optimal_models = max(_models, key=lambda x:x[1]) #select ensembles or best model \n",
        "print(f'Optimal models chosen: {optimal_models}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "l8TL72xSG2ih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls {args.directory}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpjIrLT7G2ik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_models= [os.path.join(args.directory,i) for i in os.listdir(args.directory) if args.model_name in i]\n",
        "all_models = sorted(all_models, key = lambda x: int(x[8])) #sort by epoch num.\n",
        "all_models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fya481n6G2im",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "selected_models = [all_models[i] for i in optimal_models[0]]\n",
        "pprint.pprint(selected_models)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLdc2BW3G2iq",
        "colab_type": "text"
      },
      "source": [
        "## Loading test set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pN8JtlJNG2it",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df = data_df #test set is already present in the csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3p4nzTM8G2iz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DanekyzcG2i1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataset = dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DoISn3hG2i3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataset.set_split('test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poqOKf2fG2i4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataset._target_df.split.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5AQRrWaG2i6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_state = general_utils.make_train_state() \n",
        "test_dataset.set_split('test')\n",
        "eval_bar = notebook.tqdm(\n",
        "    desc = 'split=train ',\n",
        "    total=test_dataset.get_num_batches(args.batch_size),\n",
        "    position=0,\n",
        "    leave=True,\n",
        ")\n",
        "model.eval()\n",
        "for m in notebook.tqdm(selected_models, total=len(selected_models)):\n",
        "    eval_bar.reset(\n",
        "        total=test_dataset.get_num_batches(args.batch_size),\n",
        "    )\n",
        "    model.load_state_dict(torch.load(m)['model'])\n",
        "    batch_generator = generate_batches(\n",
        "        dataset= test_dataset, batch_size= args.batch_size, shuffle=False,\n",
        "        device = args.device, drop_last=False,\n",
        "        pinned_memory = True, n_workers = 1, \n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\n",
        "            y_pred = model(batch_dict[\"x_data\"])\n",
        "\n",
        "            y_pred = y_pred.detach()\n",
        "            \n",
        "            batch_dict['y_target'] = batch_dict['y_target'].cpu()\n",
        "            test_state['batch_preds'].append(y_pred.cpu())\n",
        "            test_state['batch_targets'].append(batch_dict['y_target'].cpu())\n",
        "            test_state['batch_indexes'].append(batch_dict['x_index'].cpu())\n",
        "            eval_bar.update()\n",
        "\n",
        "    test_state['val_preds'].append(\n",
        "        torch.cat(test_state['batch_preds']).cpu()\n",
        "    )\n",
        "    test_state['val_targets'].append(\n",
        "        torch.cat(test_state['batch_targets']).cpu()\n",
        "    )\n",
        "    test_state['val_indexes'].append(\n",
        "        torch.cat(test_state['batch_indexes']).cpu()\n",
        "    )\n",
        "    \n",
        "    test_state['batch_preds'] = []\n",
        "    test_state['batch_targets'] = []\n",
        "    test_state['batch_indexes'] = []\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBLNs164G2i7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert len(test_state['val_preds']) == len(optimal_models[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tPSiWo8bjPR",
        "colab_type": "text"
      },
      "source": [
        "### Add the last layer outputs and apply argmax "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiiCZ-VyG2i-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ensemble = torch.zeros_like(test_state['val_preds'][-1])\n",
        "for i in test_state['val_preds']:\n",
        "    ensemble += i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqINUy7gG2jD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_preds = torch.argmax(ensemble, dim=1).tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "fctK1svvG2jF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "collections.Counter(test_preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrwD7ZE_G2jI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# task_b_label_dict = {'NGEN':0, 'GEN':1} #ref Reading TRAC2020 data... ipynb\n",
        "int_to_label = {0:'NGEN', 1:'GEN'}\n",
        "pred_labels = [int_to_label[i] for i in test_preds]\n",
        "collections.Counter(pred_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nJahLYRG2jJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_df = pd.DataFrame( data= {'id':test_df.ID, 'label':pred_labels})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyQiODTBG2jL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_analysis_df = pd.DataFrame( data= {'id':test_df.ID, 'text':test_df.Text ,'label':pred_labels})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "1C3S0zUDG2jN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZ2RV3NWG2jO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_analysis_df"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}